{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNcmjpsQn8RV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import chi2_contingency, shapiro, kstest\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.feature_selection import f_classif, f_regression, chi2, mutual_info_classif\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "import warnings\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configure settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AdvancedEDA:\n",
        "    \"\"\"\n",
        "    Professional-grade Exploratory Data Analysis Framework\n",
        "\n",
        "    Features:\n",
        "    - Comprehensive data profiling and quality assessment\n",
        "    - Advanced statistical analysis and hypothesis testing\n",
        "    - Multiple outlier detection methods\n",
        "    - Multicollinearity analysis with VIF\n",
        "    - Feature importance and selection\n",
        "    - Interactive visualizations\n",
        "    - Automated insights and recommendations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha: float = 0.05, figsize: Tuple[int, int] = (12, 8)):\n",
        "        \"\"\"\n",
        "        Initialize the EDA framework\n",
        "\n",
        "        Args:\n",
        "            alpha: Statistical significance level\n",
        "            figsize: Default figure size for plots\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.figsize = figsize\n",
        "        self.data = None\n",
        "        self.target_column = None\n",
        "        self.numerical_cols = []\n",
        "        self.categorical_cols = []\n",
        "        self.results = {}\n",
        "\n",
        "        # Create output directory\n",
        "        self.output_dir = Path(\"eda_output\")\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        (self.output_dir / \"plots\").mkdir(exist_ok=True)\n",
        "\n",
        "        logger.info(\"Advanced EDA Framework initialized successfully\")\n",
        "\n",
        "    def load_data(self, data_source: Union[str, pd.DataFrame], target_column: str = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and validate dataset\n",
        "\n",
        "        Args:\n",
        "            data_source: File path or DataFrame\n",
        "            target_column: Name of target variable\n",
        "\n",
        "        Returns:\n",
        "            Loaded and validated DataFrame\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(data_source, str):\n",
        "                if data_source.endswith('.csv'):\n",
        "                    self.data = pd.read_csv(data_source)\n",
        "                elif data_source.endswith(('.xlsx', '.xls')):\n",
        "                    self.data = pd.read_excel(data_source)\n",
        "                else:\n",
        "                    raise ValueError(\"Unsupported file format. Use CSV or Excel files.\")\n",
        "            elif isinstance(data_source, pd.DataFrame):\n",
        "                self.data = data_source.copy()\n",
        "            else:\n",
        "                raise ValueError(\"Data source must be file path or pandas DataFrame\")\n",
        "\n",
        "            self.target_column = target_column\n",
        "            self._identify_column_types()\n",
        "\n",
        "            logger.info(f\"Dataset loaded: {self.data.shape} | Target: {target_column}\")\n",
        "            return self.data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _identify_column_types(self):\n",
        "        \"\"\"Automatically identify numerical and categorical columns\"\"\"\n",
        "        if self.data is None:\n",
        "            raise ValueError(\"No data loaded\")\n",
        "\n",
        "        # Identify column types\n",
        "        self.numerical_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        self.categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        # Remove target from feature lists\n",
        "        if self.target_column:\n",
        "            if self.target_column in self.numerical_cols:\n",
        "                self.numerical_cols.remove(self.target_column)\n",
        "            elif self.target_column in self.categorical_cols:\n",
        "                self.categorical_cols.remove(self.target_column)\n",
        "\n",
        "        logger.info(f\"Columns identified - Numerical: {len(self.numerical_cols)}, Categorical: {len(self.categorical_cols)}\")\n",
        "\n",
        "    def data_overview(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate comprehensive data overview\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with data profile information\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            raise ValueError(\"No data loaded\")\n",
        "\n",
        "        # Basic information\n",
        "        overview = {\n",
        "            'shape': self.data.shape,\n",
        "            'memory_usage_mb': self.data.memory_usage(deep=True).sum() / (1024**2),\n",
        "            'missing_values': self.data.isnull().sum().to_dict(),\n",
        "            'missing_percentage': (self.data.isnull().sum() / len(self.data) * 100).round(2).to_dict(),\n",
        "            'duplicates': self.data.duplicated().sum(),\n",
        "            'duplicate_percentage': (self.data.duplicated().sum() / len(self.data) * 100).round(2)\n",
        "        }\n",
        "\n",
        "        # Data types summary\n",
        "        overview['column_types'] = {\n",
        "            'numerical': self.numerical_cols,\n",
        "            'categorical': self.categorical_cols,\n",
        "            'target': self.target_column\n",
        "        }\n",
        "\n",
        "        # Statistical summary for numerical columns\n",
        "        if self.numerical_cols:\n",
        "            overview['numerical_summary'] = self.data[self.numerical_cols].describe().round(3)\n",
        "\n",
        "        # Categorical summary\n",
        "        if self.categorical_cols:\n",
        "            cat_summary = {}\n",
        "            for col in self.categorical_cols:\n",
        "                cat_summary[col] = {\n",
        "                    'unique_count': self.data[col].nunique(),\n",
        "                    'top_categories': self.data[col].value_counts().head().to_dict()\n",
        "                }\n",
        "            overview['categorical_summary'] = cat_summary\n",
        "\n",
        "        self.results['data_overview'] = overview\n",
        "\n",
        "        # Print summary\n",
        "        print(\"=\" * 60)\n",
        "        print(\"üìä DATA OVERVIEW SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Dataset Shape: {overview['shape']}\")\n",
        "        print(f\"Memory Usage: {overview['memory_usage_mb']:.2f} MB\")\n",
        "        print(f\"Missing Values: {sum(overview['missing_values'].values())} cells\")\n",
        "        print(f\"Duplicate Rows: {overview['duplicates']} ({overview['duplicate_percentage']:.1f}%)\")\n",
        "        print(f\"Numerical Columns: {len(self.numerical_cols)}\")\n",
        "        print(f\"Categorical Columns: {len(self.categorical_cols)}\")\n",
        "\n",
        "        if sum(overview['missing_values'].values()) > 0:\n",
        "            print(\"\\nüìã Missing Data by Column:\")\n",
        "            for col, missing in overview['missing_values'].items():\n",
        "                if missing > 0:\n",
        "                    pct = overview['missing_percentage'][col]\n",
        "                    print(f\"  {col}: {missing} ({pct:.1f}%)\")\n",
        "\n",
        "        return overview\n",
        "\n",
        "    def analyze_distributions(self, save_plots: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Comprehensive distribution analysis with statistical tests\n",
        "\n",
        "        Args:\n",
        "            save_plots: Whether to save plots\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with distribution analysis results\n",
        "        \"\"\"\n",
        "        distribution_results = {}\n",
        "\n",
        "        # Numerical distributions\n",
        "        if self.numerical_cols:\n",
        "            print(\"\\nüìà ANALYZING NUMERICAL DISTRIBUTIONS...\")\n",
        "\n",
        "            # Create subplots\n",
        "            n_cols = min(3, len(self.numerical_cols))\n",
        "            n_rows = (len(self.numerical_cols) + n_cols - 1) // n_cols\n",
        "\n",
        "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
        "            if n_rows == 1 and n_cols == 1:\n",
        "                axes = [axes]\n",
        "            elif n_rows == 1:\n",
        "                axes = axes\n",
        "            else:\n",
        "                axes = axes.flatten()\n",
        "\n",
        "            for i, col in enumerate(self.numerical_cols):\n",
        "                # Distribution plot\n",
        "                sns.histplot(data=self.data, x=col, kde=True, ax=axes[i])\n",
        "                axes[i].set_title(f'Distribution of {col}')\n",
        "\n",
        "                # Add statistics\n",
        "                mean_val = self.data[col].mean()\n",
        "                median_val = self.data[col].median()\n",
        "                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
        "                axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
        "                axes[i].legend()\n",
        "\n",
        "                # Statistical analysis\n",
        "                col_data = self.data[col].dropna()\n",
        "                skewness = stats.skew(col_data)\n",
        "                kurtosis = stats.kurtosis(col_data)\n",
        "\n",
        "                # Normality test\n",
        "                if len(col_data) > 3:\n",
        "                    if len(col_data) <= 5000:\n",
        "                        stat, p_val = shapiro(col_data)\n",
        "                        test_name = \"Shapiro-Wilk\"\n",
        "                    else:\n",
        "                        stat, p_val = kstest(col_data, 'norm')\n",
        "                        test_name = \"Kolmogorov-Smirnov\"\n",
        "\n",
        "                    is_normal = p_val > self.alpha\n",
        "                else:\n",
        "                    stat, p_val, is_normal, test_name = np.nan, np.nan, None, \"Insufficient data\"\n",
        "\n",
        "                distribution_results[col] = {\n",
        "                    'mean': mean_val,\n",
        "                    'median': median_val,\n",
        "                    'std': self.data[col].std(),\n",
        "                    'skewness': skewness,\n",
        "                    'kurtosis': kurtosis,\n",
        "                    'normality_test': test_name,\n",
        "                    'normality_p_value': p_val,\n",
        "                    'is_normal': is_normal\n",
        "                }\n",
        "\n",
        "                print(f\"  {col}: Skew={skewness:.3f}, Kurt={kurtosis:.3f}, Normal={is_normal} (p={p_val:.4f})\")\n",
        "\n",
        "            # Hide unused subplots\n",
        "            for i in range(len(self.numerical_cols), len(axes)):\n",
        "                axes[i].set_visible(False)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            if save_plots:\n",
        "                plt.savefig(self.output_dir / \"plots\" / \"numerical_distributions.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "        # Categorical distributions\n",
        "        if self.categorical_cols:\n",
        "            print(\"\\nüìä ANALYZING CATEGORICAL DISTRIBUTIONS...\")\n",
        "\n",
        "            for col in self.categorical_cols:\n",
        "                plt.figure(figsize=self.figsize)\n",
        "\n",
        "                # Get top categories (limit to 20 for readability)\n",
        "                value_counts = self.data[col].value_counts()\n",
        "                top_categories = value_counts.head(20)\n",
        "\n",
        "                # Create horizontal bar plot\n",
        "                sns.barplot(y=top_categories.index, x=top_categories.values, orient='h')\n",
        "                plt.title(f'Distribution of {col}')\n",
        "                plt.xlabel('Count')\n",
        "\n",
        "                # Add percentage labels\n",
        "                total = len(self.data)\n",
        "                for i, (category, count) in enumerate(top_categories.items()):\n",
        "                    try:\n",
        "                        percentage = (count / total) * 100\n",
        "                        plt.text(count + total*0.01, i, f'{percentage:.1f}%', va='center')\n",
        "                    except (TypeError, ZeroDivisionError):\n",
        "                        continue\n",
        "\n",
        "                plt.tight_layout()\n",
        "                if save_plots:\n",
        "                    plt.savefig(self.output_dir / \"plots\" / f\"categorical_{col}.png\", dpi=300, bbox_inches='tight')\n",
        "                plt.show()\n",
        "\n",
        "                # Store results\n",
        "                distribution_results[col] = {\n",
        "                    'unique_values': self.data[col].nunique(),\n",
        "                    'most_frequent': value_counts.index[0],\n",
        "                    'frequency': value_counts.iloc[0],\n",
        "                    'frequency_percentage': (value_counts.iloc[0] / total * 100).round(2)\n",
        "                }\n",
        "\n",
        "                print(f\"  {col}: {self.data[col].nunique()} unique values, Top: {value_counts.index[0]} ({value_counts.iloc[0]/total*100:.1f}%)\")\n",
        "\n",
        "        self.results['distributions'] = distribution_results\n",
        "        return distribution_results\n",
        "\n",
        "    def correlation_analysis(self, method: str = 'pearson', plot_heatmap: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Advanced correlation analysis with statistical significance\n",
        "\n",
        "        Args:\n",
        "            method: Correlation method ('pearson', 'spearman', 'kendall')\n",
        "            plot_heatmap: Whether to plot correlation heatmap\n",
        "\n",
        "        Returns:\n",
        "            Correlation matrix\n",
        "        \"\"\"\n",
        "        if not self.numerical_cols:\n",
        "            print(\"‚ö†Ô∏è No numerical columns for correlation analysis\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"\\nüîó CORRELATION ANALYSIS ({method.upper()})...\")\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = self.data[self.numerical_cols].corr(method=method)\n",
        "\n",
        "        if plot_heatmap:\n",
        "            plt.figure(figsize=(max(8, len(self.numerical_cols)), max(6, len(self.numerical_cols))))\n",
        "\n",
        "            # Create mask for upper triangle\n",
        "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "            # Generate heatmap\n",
        "            sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "                       square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, fmt='.2f')\n",
        "\n",
        "            plt.title(f'{method.capitalize()} Correlation Matrix')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(self.output_dir / \"plots\" / f\"correlation_{method}.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "        # Find high correlations\n",
        "        high_corr_pairs = []\n",
        "        threshold = 0.7\n",
        "\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "                if abs(corr_val) > threshold:\n",
        "                    high_corr_pairs.append({\n",
        "                        'var1': corr_matrix.columns[i],\n",
        "                        'var2': corr_matrix.columns[j],\n",
        "                        'correlation': corr_val\n",
        "                    })\n",
        "\n",
        "        if high_corr_pairs:\n",
        "            print(f\"üîç High correlations found (|r| > {threshold}):\")\n",
        "            for pair in high_corr_pairs:\n",
        "                print(f\"  {pair['var1']} ‚Üî {pair['var2']}: {pair['correlation']:.3f}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ No high correlations detected (threshold: {threshold})\")\n",
        "\n",
        "        self.results['correlation_matrix'] = corr_matrix\n",
        "        self.results['high_correlations'] = high_corr_pairs\n",
        "\n",
        "        return corr_matrix\n",
        "\n",
        "    def outlier_detection(self, methods: List[str] = ['iqr', 'zscore', 'isolation']) -> Dict:\n",
        "        \"\"\"\n",
        "        Multi-method outlier detection\n",
        "\n",
        "        Args:\n",
        "            methods: List of detection methods\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with outlier detection results\n",
        "        \"\"\"\n",
        "        print(f\"\\nüéØ OUTLIER DETECTION using {', '.join(methods).upper()}...\")\n",
        "\n",
        "        outlier_results = {}\n",
        "\n",
        "        for col in self.numerical_cols:\n",
        "            outlier_results[col] = {}\n",
        "            col_data = self.data[col].dropna()\n",
        "\n",
        "            for method in methods:\n",
        "                if method == 'iqr':\n",
        "                    Q1 = col_data.quantile(0.25)\n",
        "                    Q3 = col_data.quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - 1.5 * IQR\n",
        "                    upper_bound = Q3 + 1.5 * IQR\n",
        "                    outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
        "\n",
        "                elif method == 'zscore':\n",
        "                    z_scores = np.abs(stats.zscore(col_data))\n",
        "                    outliers = col_data[z_scores > 3]\n",
        "\n",
        "                elif method == 'isolation':\n",
        "                    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "                    outlier_pred = iso_forest.fit_predict(col_data.values.reshape(-1, 1))\n",
        "                    outliers = col_data[outlier_pred == -1]\n",
        "\n",
        "                outlier_results[col][method] = {\n",
        "                    'count': len(outliers),\n",
        "                    'percentage': (len(outliers) / len(col_data)) * 100,\n",
        "                    'indices': outliers.index.tolist()\n",
        "                }\n",
        "\n",
        "        # Visualization\n",
        "        if self.numerical_cols:\n",
        "            n_cols = min(3, len(self.numerical_cols))\n",
        "            n_rows = (len(self.numerical_cols) + n_cols - 1) // n_cols\n",
        "\n",
        "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
        "            if n_rows == 1 and n_cols == 1:\n",
        "                axes = [axes]\n",
        "            elif n_rows == 1:\n",
        "                axes = axes\n",
        "            else:\n",
        "                axes = axes.flatten()\n",
        "\n",
        "            for i, col in enumerate(self.numerical_cols):\n",
        "                sns.boxplot(data=self.data, y=col, ax=axes[i])\n",
        "                outlier_count = outlier_results[col]['iqr']['count']\n",
        "                outlier_pct = outlier_results[col]['iqr']['percentage']\n",
        "                axes[i].set_title(f'{col}\\nOutliers: {outlier_count} ({outlier_pct:.1f}%)')\n",
        "\n",
        "            # Hide unused subplots\n",
        "            for i in range(len(self.numerical_cols), len(axes)):\n",
        "                axes[i].set_visible(False)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(self.output_dir / \"plots\" / \"outlier_detection.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "        # Summary\n",
        "        print(\"üìã Outlier Detection Summary:\")\n",
        "        for col in self.numerical_cols:\n",
        "            results = outlier_results[col]\n",
        "            print(f\"  {col}:\")\n",
        "            for method in methods:\n",
        "                count = results[method]['count']\n",
        "                pct = results[method]['percentage']\n",
        "                print(f\"    {method.upper()}: {count} outliers ({pct:.1f}%)\")\n",
        "\n",
        "        self.results['outliers'] = outlier_results\n",
        "        return outlier_results\n",
        "\n",
        "    def multicollinearity_check(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Variance Inflation Factor (VIF) analysis for multicollinearity\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with VIF values\n",
        "        \"\"\"\n",
        "        if len(self.numerical_cols) < 2:\n",
        "            print(\"‚ö†Ô∏è Need at least 2 numerical columns for VIF analysis\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(\"\\nüîç MULTICOLLINEARITY ANALYSIS (VIF)...\")\n",
        "\n",
        "        # Prepare data\n",
        "        X = self.data[self.numerical_cols].dropna()\n",
        "        X_with_const = add_constant(X)\n",
        "\n",
        "        # Calculate VIF\n",
        "        vif_data = pd.DataFrame()\n",
        "        vif_data['Feature'] = X_with_const.columns\n",
        "        vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i)\n",
        "                          for i in range(X_with_const.shape[1])]\n",
        "\n",
        "        vif_data = vif_data.sort_values('VIF', ascending=False)\n",
        "\n",
        "        # Visualization\n",
        "        plt.figure(figsize=self.figsize)\n",
        "        features_to_plot = vif_data[vif_data['Feature'] != 'const']  # Exclude constant\n",
        "        sns.barplot(data=features_to_plot, y='Feature', x='VIF')\n",
        "        plt.axvline(x=5, color='red', linestyle='--', label='VIF = 5 (Threshold)')\n",
        "        plt.axvline(x=10, color='orange', linestyle='--', label='VIF = 10 (High)')\n",
        "        plt.title('Variance Inflation Factor (VIF) Analysis')\n",
        "        plt.xlabel('VIF Score')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / \"plots\" / \"vif_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Analysis\n",
        "        high_vif = features_to_plot[features_to_plot['VIF'] > 5]\n",
        "        if not high_vif.empty:\n",
        "            print(\"‚ö†Ô∏è High multicollinearity detected (VIF > 5):\")\n",
        "            for _, row in high_vif.iterrows():\n",
        "                print(f\"  {row['Feature']}: VIF = {row['VIF']:.2f}\")\n",
        "\n",
        "            print(\"\\nüí° Recommendations:\")\n",
        "            print(\"  ‚Ä¢ Remove highly correlated features\")\n",
        "            print(\"  ‚Ä¢ Apply Principal Component Analysis (PCA)\")\n",
        "            print(\"  ‚Ä¢ Use regularization techniques (Ridge/Lasso)\")\n",
        "        else:\n",
        "            print(\"‚úÖ No multicollinearity issues detected (all VIF < 5)\")\n",
        "\n",
        "        self.results['vif_analysis'] = vif_data\n",
        "        return vif_data\n",
        "\n",
        "    def bivariate_analysis(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Comprehensive bivariate analysis with target variable\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with bivariate analysis results\n",
        "        \"\"\"\n",
        "        if not self.target_column:\n",
        "            print(\"‚ö†Ô∏è No target column specified for bivariate analysis\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"\\nüî¨ BIVARIATE ANALYSIS with target: {self.target_column}\")\n",
        "\n",
        "        # Validate target column\n",
        "        if self.target_column not in self.data.columns:\n",
        "            print(f\"‚ùå Target column '{self.target_column}' not found in data\")\n",
        "            return {}\n",
        "\n",
        "        bivariate_results = {}\n",
        "        target_data = self.data[self.target_column]\n",
        "\n",
        "        # Ensure target column exists and has valid data\n",
        "        if target_data.empty or target_data.isnull().all():\n",
        "            print(\"‚ùå Target column is empty or all null values\")\n",
        "            return bivariate_results\n",
        "\n",
        "        target_type = 'categorical' if self.target_column in self.categorical_cols else 'numerical'\n",
        "        print(f\"Target type detected: {target_type}\")\n",
        "\n",
        "        # Numerical features vs target\n",
        "        for col in self.numerical_cols:\n",
        "            try:\n",
        "                # Ensure column data is numerical\n",
        "                col_data = pd.to_numeric(self.data[col], errors='coerce')\n",
        "                if col_data.isnull().all():\n",
        "                    print(f\"  Skipping {col}: Cannot convert to numerical data\")\n",
        "                    continue\n",
        "\n",
        "                if target_type == 'categorical':\n",
        "                    # Box plots and statistical tests\n",
        "                    plt.figure(figsize=self.figsize)\n",
        "                    sns.boxplot(data=self.data, x=self.target_column, y=col)\n",
        "                    plt.title(f'{col} by {self.target_column}')\n",
        "                    plt.xticks(rotation=45)\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(self.output_dir / \"plots\" / f\"bivariate_{col}_vs_{self.target_column}.png\",\n",
        "                               dpi=300, bbox_inches='tight')\n",
        "                    plt.show()\n",
        "\n",
        "                    # Statistical test\n",
        "                    try:\n",
        "                        groups = []\n",
        "                        for name, group in self.data.groupby(self.target_column):\n",
        "                            group_data = group[col].dropna()\n",
        "                            if len(group_data) > 0:\n",
        "                                groups.append(group_data)\n",
        "\n",
        "                        if len(groups) == 2 and all(len(g) > 0 for g in groups):\n",
        "                            stat, p_val = stats.ttest_ind(groups[0], groups[1], equal_var=False)\n",
        "                            test_name = \"Welch's t-test\"\n",
        "                        elif len(groups) > 2 and all(len(g) > 0 for g in groups):\n",
        "                            stat, p_val = stats.f_oneway(*groups)\n",
        "                            test_name = \"ANOVA\"\n",
        "                        else:\n",
        "                            stat, p_val, test_name = np.nan, np.nan, \"Insufficient data\"\n",
        "                    except Exception as e:\n",
        "                        stat, p_val, test_name = np.nan, np.nan, f\"Test failed: {str(e)}\"\n",
        "\n",
        "                    bivariate_results[col] = {\n",
        "                        'test': test_name,\n",
        "                        'statistic': stat,\n",
        "                        'p_value': p_val,\n",
        "                        'significant': p_val < self.alpha if not np.isnan(p_val) else False\n",
        "                    }\n",
        "\n",
        "                    significance = \"‚úÖ Significant\" if p_val < self.alpha else \"‚ùå Not significant\"\n",
        "                    print(f\"  {col} vs {self.target_column}: {test_name}, p={p_val:.4f} - {significance}\")\n",
        "\n",
        "                else:  # Numerical target\n",
        "                    # Scatter plot with correlation\n",
        "                    plt.figure(figsize=self.figsize)\n",
        "                    sns.scatterplot(data=self.data, x=col, y=self.target_column, alpha=0.6)\n",
        "                    sns.regplot(data=self.data, x=col, y=self.target_column, scatter=False, color='red')\n",
        "\n",
        "                    # Calculate correlation\n",
        "                    try:\n",
        "                        corr_val = self.data[col].corr(target_data)\n",
        "                        if pd.isna(corr_val):\n",
        "                            corr_val = 0.0\n",
        "                    except (TypeError, ValueError):\n",
        "                        corr_val = 0.0\n",
        "\n",
        "                    plt.title(f'{col} vs {self.target_column} (r = {corr_val:.3f})')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(self.output_dir / \"plots\" / f\"scatter_{col}_vs_{self.target_column}.png\",\n",
        "                               dpi=300, bbox_inches='tight')\n",
        "                    plt.show()\n",
        "\n",
        "                    bivariate_results[col] = {\n",
        "                        'correlation': corr_val,\n",
        "                        'correlation_strength': self._interpret_correlation(abs(corr_val))\n",
        "                    }\n",
        "\n",
        "                    print(f\"  {col} vs {self.target_column}: r = {corr_val:.3f} - {self._interpret_correlation(abs(corr_val))}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error analyzing {col}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Categorical features vs target\n",
        "        for col in self.categorical_cols:\n",
        "            if col != self.target_column:\n",
        "                try:\n",
        "                    if target_type == 'categorical':\n",
        "                        # Cross-tabulation and Chi-square test\n",
        "                        ct = pd.crosstab(self.data[col], target_data)\n",
        "\n",
        "                        plt.figure(figsize=self.figsize)\n",
        "                        sns.heatmap(ct, annot=True, fmt='d', cmap='Blues')\n",
        "                        plt.title(f'Cross-tabulation: {col} vs {self.target_column}')\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(self.output_dir / \"plots\" / f\"crosstab_{col}_vs_{self.target_column}.png\",\n",
        "                                   dpi=300, bbox_inches='tight')\n",
        "                        plt.show()\n",
        "\n",
        "                        # Chi-square test\n",
        "                        try:\n",
        "                            if ct.size > 0 and ct.sum().sum() > 0:\n",
        "                                chi2, p_val, dof, expected = chi2_contingency(ct)\n",
        "                                # Cram√©r's V\n",
        "                                n = ct.sum().sum()\n",
        "                                min_dim = min(ct.shape) - 1\n",
        "                                if min_dim > 0:\n",
        "                                    cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
        "                                else:\n",
        "                                    cramers_v = 0.0\n",
        "\n",
        "                                bivariate_results[col] = {\n",
        "                                    'test': 'Chi-square',\n",
        "                                    'statistic': chi2,\n",
        "                                    'p_value': p_val,\n",
        "                                    'cramers_v': cramers_v,\n",
        "                                    'significant': p_val < self.alpha\n",
        "                                }\n",
        "\n",
        "                                significance = \"‚úÖ Significant\" if p_val < self.alpha else \"‚ùå Not significant\"\n",
        "                                print(f\"  {col} vs {self.target_column}: Chi-square, p={p_val:.4f}, Cram√©r's V={cramers_v:.3f} - {significance}\")\n",
        "                            else:\n",
        "                                print(f\"  {col} vs {self.target_column}: Empty cross-tabulation\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"  {col} vs {self.target_column}: Chi-square test failed - {str(e)}\")\n",
        "\n",
        "                    else:  # Numerical target\n",
        "                        # Box plot by category\n",
        "                        plt.figure(figsize=self.figsize)\n",
        "                        sns.boxplot(data=self.data, x=col, y=self.target_column)\n",
        "                        plt.title(f'{self.target_column} by {col}')\n",
        "                        plt.xticks(rotation=45)\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(self.output_dir / \"plots\" / f\"boxplot_{self.target_column}_by_{col}.png\",\n",
        "                                   dpi=300, bbox_inches='tight')\n",
        "                        plt.show()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error analyzing {col}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        self.results['bivariate_analysis'] = bivariate_results\n",
        "        return bivariate_results\n",
        "\n",
        "    def _interpret_correlation(self, corr_val: float) -> str:\n",
        "        \"\"\"Interpret correlation strength\"\"\"\n",
        "        if corr_val >= 0.7:\n",
        "            return \"Strong\"\n",
        "        elif corr_val >= 0.5:\n",
        "            return \"Moderate\"\n",
        "        elif corr_val >= 0.3:\n",
        "            return \"Weak\"\n",
        "        else:\n",
        "            return \"Very weak\"\n",
        "\n",
        "    def feature_importance(self, method: str = 'auto') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculate feature importance scores\n",
        "\n",
        "        Args:\n",
        "            method: Feature selection method ('auto', 'f_test', 'chi2', 'mutual_info')\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with feature importance scores\n",
        "        \"\"\"\n",
        "        if not self.target_column:\n",
        "            print(\"‚ö†Ô∏è No target column specified for feature importance\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"\\nüéØ FEATURE IMPORTANCE ANALYSIS...\")\n",
        "\n",
        "        # Prepare data\n",
        "        X = self.data.drop(columns=[self.target_column])\n",
        "        y = self.data[self.target_column]\n",
        "\n",
        "        # Encode categorical variables\n",
        "        X_encoded = X.copy()\n",
        "        y_encoded = y.copy()\n",
        "\n",
        "        # Handle categorical features\n",
        "        for col in X_encoded.select_dtypes(include=['object', 'category']).columns:\n",
        "            try:\n",
        "                le = LabelEncoder()\n",
        "                X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not encode column {col}: {str(e)}\")\n",
        "                X_encoded = X_encoded.drop(columns=[col])\n",
        "\n",
        "        # Handle categorical target\n",
        "        if y_encoded.dtype in ['object', 'category']:\n",
        "            try:\n",
        "                le_y = LabelEncoder()\n",
        "                y_encoded = le_y.fit_transform(y_encoded.astype(str))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not encode target variable: {str(e)}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        # Auto-select method\n",
        "        if method == 'auto':\n",
        "            if y.dtype in ['object', 'category'] or y.nunique() < 20:\n",
        "                method = 'f_classif' if len(self.numerical_cols) > 0 else 'chi2'\n",
        "            else:\n",
        "                method = 'f_regression'\n",
        "\n",
        "        # Calculate scores\n",
        "        try:\n",
        "            if method == 'f_classif':\n",
        "                scores, p_values = f_classif(X_encoded, y_encoded)\n",
        "            elif method == 'f_regression':\n",
        "                scores, p_values = f_regression(X_encoded, y_encoded)\n",
        "            elif method == 'chi2':\n",
        "                # Ensure non-negative values\n",
        "                X_encoded = X_encoded - X_encoded.min() + 1\n",
        "                scores, p_values = chi2(X_encoded, y_encoded)\n",
        "            elif method == 'mutual_info':\n",
        "                scores = mutual_info_classif(X_encoded, y_encoded, random_state=42)\n",
        "                p_values = np.full_like(scores, np.nan)\n",
        "\n",
        "            # Create results DataFrame\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance_score': scores,\n",
        "                'p_value': p_values\n",
        "            }).sort_values('importance_score', ascending=False)\n",
        "\n",
        "            # Visualization\n",
        "            plt.figure(figsize=self.figsize)\n",
        "            top_features = importance_df.head(min(15, len(importance_df)))\n",
        "            sns.barplot(data=top_features, y='feature', x='importance_score')\n",
        "            plt.title(f'Feature Importance ({method})')\n",
        "            plt.xlabel('Importance Score')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(self.output_dir / \"plots\" / \"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            # Summary\n",
        "            print(f\"üìä Top 10 Most Important Features ({method}):\")\n",
        "            for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
        "                p_str = f\" (p={row['p_value']:.4f})\" if not pd.isna(row['p_value']) else \"\"\n",
        "                print(f\"  {i:2d}. {row['feature']:<20} Score: {row['importance_score']:.3f}{p_str}\")\n",
        "\n",
        "            if not importance_df['p_value'].isna().all():\n",
        "                significant_features = importance_df[importance_df['p_value'] < self.alpha]\n",
        "                print(f\"\\n‚úÖ {len(significant_features)} features are statistically significant (p < {self.alpha})\")\n",
        "\n",
        "            self.results['feature_importance'] = importance_df\n",
        "            return importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Feature importance analysis failed: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def preprocess_data(self,\n",
        "                       handle_missing: str = 'drop',\n",
        "                       encode_categorical: str = 'onehot',\n",
        "                       scale_numerical: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Advanced data preprocessing pipeline\n",
        "\n",
        "        Args:\n",
        "            handle_missing: Strategy for missing values ('drop', 'impute')\n",
        "            encode_categorical: Categorical encoding ('onehot', 'label')\n",
        "            scale_numerical: Whether to scale numerical features\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed DataFrame\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîß DATA PREPROCESSING...\")\n",
        "        print(f\"  Missing value strategy: {handle_missing}\")\n",
        "        print(f\"  Categorical encoding: {encode_categorical}\")\n",
        "        print(f\"  Numerical scaling: {scale_numerical}\")\n",
        "\n",
        "        df_processed = self.data.copy()\n",
        "\n",
        "        # Handle missing values\n",
        "        if handle_missing == 'drop':\n",
        "            # Drop columns with >50% missing values\n",
        "            high_missing_cols = df_processed.columns[df_processed.isnull().mean() > 0.5]\n",
        "            if len(high_missing_cols) > 0:\n",
        "                df_processed = df_processed.drop(columns=high_missing_cols)\n",
        "                print(f\"  Dropped {len(high_missing_cols)} columns with >50% missing values\")\n",
        "\n",
        "            # Drop rows with any missing values\n",
        "            initial_rows = len(df_processed)\n",
        "            df_processed = df_processed.dropna()\n",
        "            dropped_rows = initial_rows - len(df_processed)\n",
        "            print(f\"  Dropped {dropped_rows} rows with missing values\")\n",
        "\n",
        "        elif handle_missing == 'impute':\n",
        "            # Impute numerical columns with median\n",
        "            for col in self.numerical_cols:\n",
        "                if col in df_processed.columns:\n",
        "                    df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "\n",
        "            # Impute categorical columns with mode\n",
        "            for col in self.categorical_cols:\n",
        "                if col in df_processed.columns and not df_processed[col].mode().empty:\n",
        "                    df_processed[col].fillna(df_processed[col].mode().iloc[0], inplace=True)\n",
        "\n",
        "            print(f\"  Imputed missing values for all columns\")\n",
        "\n",
        "        # Encode categorical variables\n",
        "        if encode_categorical == 'onehot':\n",
        "            categorical_cols_to_encode = [col for col in self.categorical_cols if col in df_processed.columns]\n",
        "            if categorical_cols_to_encode:\n",
        "                df_processed = pd.get_dummies(df_processed, columns=categorical_cols_to_encode, drop_first=True)\n",
        "                print(f\"  One-hot encoded {len(categorical_cols_to_encode)} categorical columns\")\n",
        "\n",
        "        elif encode_categorical == 'label':\n",
        "            for col in self.categorical_cols:\n",
        "                if col in df_processed.columns:\n",
        "                    le = LabelEncoder()\n",
        "                    df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "            print(f\"  Label encoded {len(self.categorical_cols)} categorical columns\")\n",
        "\n",
        "        # Scale numerical features\n",
        "        if scale_numerical and self.numerical_cols:\n",
        "            scaler = StandardScaler()\n",
        "            numerical_cols_to_scale = [col for col in self.numerical_cols if col in df_processed.columns]\n",
        "            if numerical_cols_to_scale:\n",
        "                df_processed[numerical_cols_to_scale] = scaler.fit_transform(df_processed[numerical_cols_to_scale])\n",
        "                print(f\"  Standardized {len(numerical_cols_to_scale)} numerical columns\")\n",
        "\n",
        "        print(f\"  Final shape: {df_processed.shape}\")\n",
        "\n",
        "        self.results['preprocessed_data'] = df_processed\n",
        "        return df_processed\n",
        "\n",
        "    def generate_insights(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate automated insights and recommendations\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with insights and recommendations\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ü§ñ AUTOMATED INSIGHTS & RECOMMENDATIONS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        insights = {\n",
        "            'data_quality': [],\n",
        "            'distributions': [],\n",
        "            'correlations': [],\n",
        "            'outliers': [],\n",
        "            'feature_selection': [],\n",
        "            'modeling_recommendations': []\n",
        "        }\n",
        "\n",
        "        # Data Quality Insights\n",
        "        if 'data_overview' in self.results:\n",
        "            overview = self.results['data_overview']\n",
        "\n",
        "            missing_total = sum(overview['missing_values'].values())\n",
        "            missing_pct = (missing_total / (self.data.shape[0] * self.data.shape[1])) * 100\n",
        "\n",
        "            if missing_pct == 0:\n",
        "                insights['data_quality'].append(\"‚úÖ Excellent data quality - no missing values\")\n",
        "            elif missing_pct < 5:\n",
        "                insights['data_quality'].append(f\"‚úÖ Good data quality - only {missing_pct:.1f}% missing values\")\n",
        "                insights['data_quality'].append(\"üí° Simple imputation strategies recommended\")\n",
        "            elif missing_pct < 20:\n",
        "                insights['data_quality'].append(f\"‚ö†Ô∏è Moderate missing data - {missing_pct:.1f}% missing values\")\n",
        "                insights['data_quality'].append(\"üí° Consider advanced imputation (KNN, iterative)\")\n",
        "            else:\n",
        "                insights['data_quality'].append(f\"‚ùå High missing data - {missing_pct:.1f}% missing values\")\n",
        "                insights['data_quality'].append(\"üí° Investigate data collection process\")\n",
        "\n",
        "            if overview['duplicate_percentage'] > 5:\n",
        "                insights['data_quality'].append(f\"‚ö†Ô∏è {overview['duplicate_percentage']:.1f}% duplicate rows detected\")\n",
        "                insights['data_quality'].append(\"üí° Remove duplicates before modeling\")\n",
        "\n",
        "        # Distribution Insights\n",
        "        if 'distributions' in self.results:\n",
        "            distributions = self.results['distributions']\n",
        "            skewed_features = []\n",
        "            non_normal_features = []\n",
        "\n",
        "            for col, stats in distributions.items():\n",
        "                if col in self.numerical_cols:\n",
        "                    if abs(stats.get('skewness', 0)) > 1:\n",
        "                        skewed_features.append(col)\n",
        "                    if stats.get('is_normal') == False:\n",
        "                        non_normal_features.append(col)\n",
        "\n",
        "            if skewed_features:\n",
        "                insights['distributions'].append(f\"‚ö†Ô∏è Highly skewed features: {', '.join(skewed_features)}\")\n",
        "                insights['distributions'].append(\"üí° Consider log/Box-Cox transformations\")\n",
        "\n",
        "            if non_normal_features:\n",
        "                insights['distributions'].append(f\"üìä Non-normal distributions: {len(non_normal_features)} features\")\n",
        "                insights['distributions'].append(\"üí° Consider non-parametric methods or transformations\")\n",
        "\n",
        "        # Correlation Insights\n",
        "        if 'high_correlations' in self.results:\n",
        "            high_corr = self.results['high_correlations']\n",
        "            if high_corr:\n",
        "                insights['correlations'].append(f\"‚ö†Ô∏è {len(high_corr)} high correlation pairs detected\")\n",
        "                insights['correlations'].append(\"üí° Consider feature selection or PCA\")\n",
        "                insights['correlations'].append(\"üí° Use regularized models (Ridge/Lasso)\")\n",
        "            else:\n",
        "                insights['correlations'].append(\"‚úÖ No problematic correlations detected\")\n",
        "\n",
        "        # Outlier Insights\n",
        "        if 'outliers' in self.results:\n",
        "            outliers = self.results['outliers']\n",
        "            total_outliers = sum([result['iqr']['count'] for result in outliers.values()])\n",
        "            outlier_rate = (total_outliers / (self.data.shape[0] * len(self.numerical_cols))) * 100\n",
        "\n",
        "            if outlier_rate > 10:\n",
        "                insights['outliers'].append(f\"‚ö†Ô∏è High outlier rate: {outlier_rate:.1f}%\")\n",
        "                insights['outliers'].append(\"üí° Investigate outliers, use robust methods\")\n",
        "            elif outlier_rate > 5:\n",
        "                insights['outliers'].append(f\"‚ö†Ô∏è Moderate outlier rate: {outlier_rate:.1f}%\")\n",
        "                insights['outliers'].append(\"üí° Monitor model performance with/without outliers\")\n",
        "            else:\n",
        "                insights['outliers'].append(f\"‚úÖ Low outlier rate: {outlier_rate:.1f}%\")\n",
        "\n",
        "        # Feature Selection Insights\n",
        "        if 'feature_importance' in self.results:\n",
        "            importance = self.results['feature_importance']\n",
        "            if not importance.empty:\n",
        "                top_features = len(importance[importance['importance_score'] > importance['importance_score'].median()])\n",
        "                insights['feature_selection'].append(f\"üìä {top_features} features above median importance\")\n",
        "\n",
        "                if not importance['p_value'].isna().all():\n",
        "                    significant = len(importance[importance['p_value'] < self.alpha])\n",
        "                    insights['feature_selection'].append(f\"‚úÖ {significant} statistically significant features\")\n",
        "\n",
        "        # Modeling Recommendations\n",
        "        if len(self.categorical_cols) > len(self.numerical_cols):\n",
        "            insights['modeling_recommendations'].append(\"üìä Primarily categorical data detected\")\n",
        "            insights['modeling_recommendations'].append(\"üí° Consider: Random Forest, XGBoost, CatBoost\")\n",
        "        else:\n",
        "            insights['modeling_recommendations'].append(\"üìä Primarily numerical data detected\")\n",
        "            insights['modeling_recommendations'].append(\"üí° Consider: Linear models, SVM, Neural Networks\")\n",
        "\n",
        "        if self.data.shape[0] < 1000:\n",
        "            insights['modeling_recommendations'].append(\"üìè Small dataset - use cross-validation\")\n",
        "            insights['modeling_recommendations'].append(\"üí° Avoid overly complex models\")\n",
        "        elif self.data.shape[0] > 100000:\n",
        "            insights['modeling_recommendations'].append(\"üìè Large dataset - consider sampling for EDA\")\n",
        "            insights['modeling_recommendations'].append(\"üí° Use scalable algorithms\")\n",
        "\n",
        "        # Print insights\n",
        "        for category, insight_list in insights.items():\n",
        "            if insight_list:\n",
        "                print(f\"\\n{category.replace('_', ' ').upper()}:\")\n",
        "                for insight in insight_list:\n",
        "                    print(f\"  {insight}\")\n",
        "\n",
        "        print(\"\\nüéØ NEXT STEPS:\")\n",
        "        print(\"1. Address identified data quality issues\")\n",
        "        print(\"2. Apply recommended transformations\")\n",
        "        print(\"3. Select important features\")\n",
        "        print(\"4. Choose appropriate modeling approach\")\n",
        "        print(\"5. Implement proper validation strategy\")\n",
        "\n",
        "        self.results['insights'] = insights\n",
        "        return insights\n",
        "\n",
        "    def run_complete_analysis(self, save_plots: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Run complete EDA analysis pipeline\n",
        "\n",
        "        Args:\n",
        "            save_plots: Whether to save plots\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all analysis results\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            raise ValueError(\"No data loaded. Please load data first.\")\n",
        "\n",
        "        print(\"üöÄ RUNNING COMPLETE EDA ANALYSIS...\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        try:\n",
        "            # 1. Data Overview\n",
        "            self.data_overview()\n",
        "\n",
        "            # 2. Distribution Analysis\n",
        "            self.analyze_distributions(save_plots=save_plots)\n",
        "\n",
        "            # 3. Correlation Analysis\n",
        "            self.correlation_analysis(plot_heatmap=save_plots)\n",
        "\n",
        "            # 4. Outlier Detection\n",
        "            self.outlier_detection()\n",
        "\n",
        "            # 5. Multicollinearity Check\n",
        "            self.multicollinearity_check()\n",
        "\n",
        "            # 6. Bivariate Analysis\n",
        "            if self.target_column:\n",
        "                self.bivariate_analysis()\n",
        "\n",
        "            # 7. Feature Importance\n",
        "            if self.target_column:\n",
        "                self.feature_importance()\n",
        "\n",
        "            # 8. Generate Insights\n",
        "            self.generate_insights()\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"‚úÖ COMPLETE EDA ANALYSIS FINISHED\")\n",
        "            print(f\"üìÅ Results saved to: {self.output_dir}\")\n",
        "            if save_plots:\n",
        "                print(f\"üìä Plots saved to: {self.output_dir}/plots/\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            return self.results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in complete analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Example usage of the Advanced EDA Framework\n",
        "    \"\"\"\n",
        "    # Initialize the framework\n",
        "    eda = AdvancedEDA(alpha=0.05, figsize=(12, 8))\n",
        "\n",
        "    # Example with sample data (replace with your data)\n",
        "    try:\n",
        "        # Option 1: Load from file\n",
        "        # data = eda.load_data('your_dataset.csv', target_column='target')\n",
        "\n",
        "        # Option 2: Create sample data for demonstration\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "\n",
        "        sample_data = pd.DataFrame({\n",
        "            'age': np.random.randint(18, 80, n_samples),\n",
        "            'income': np.random.normal(50000, 15000, n_samples),\n",
        "            'education_years': np.random.randint(8, 20, n_samples),\n",
        "            'experience': np.random.randint(0, 40, n_samples),\n",
        "            'department': np.random.choice(['Sales', 'Engineering', 'Marketing', 'HR'], n_samples),\n",
        "            'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
        "            'performance': np.random.choice(['Low', 'Medium', 'High'], n_samples)\n",
        "        })\n",
        "\n",
        "        # Ensure no missing values in sample data\n",
        "        for col in sample_data.columns:\n",
        "            if sample_data[col].dtype in ['object', 'category']:\n",
        "                sample_data[col] = sample_data[col].fillna('Unknown')\n",
        "            else:\n",
        "                sample_data[col] = sample_data[col].fillna(sample_data[col].mean())\n",
        "\n",
        "        # Load data\n",
        "        data = eda.load_data(sample_data, target_column='performance')\n",
        "\n",
        "        # Run complete analysis\n",
        "        results = eda.run_complete_analysis(save_plots=True)\n",
        "\n",
        "        print(\"\\nüéâ EDA Framework demonstration completed successfully!\")\n",
        "        print(\"üìà Check the generated plots and analysis results above.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in demonstration: {str(e)}\")\n",
        "        print(\"üí° Please ensure you have all required libraries installed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}